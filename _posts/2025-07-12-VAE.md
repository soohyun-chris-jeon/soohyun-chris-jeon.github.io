---
layout: post
title:  "VAE: Variational Autoencoder"
date:   2025-07-12 11:00:00 +0900
categories: [Deep Learning, Generative Model]
tags: [VAE, Generative Model, PyTorch, CNN, convolutional-neural-network, Deep Learning, AI, Computer Vision]
comments: true     # ëŒ“ê¸€ ê¸°ëŠ¥ ì‚¬ìš© (ì˜µì…˜)
math: true  # âœ… ìš”ê±° ê¼­ ì¶”ê°€!
image:
  path: https://www.mygreatlearning.com/blog/wp-content/uploads/2020/10/variational-autoencoder-banner.jpg


---


## ğŸŸ£ Intro

ëŒ€í‘œì ì¸ generative modelì˜ ì²« ë²ˆì§¸ ì‹œë¦¬ì¦ˆì¸ VAEì— ëŒ€í•´ì„œ ì •ë¦¬í•´ë³´ë ¤ê³  í•œë‹¤.



#### âšª VAE (Variational AutoEncoder)ë€?


- VAEëŠ” *í™•ë¥ ì  latent space*ë¥¼ í•™ìŠµí•˜ëŠ” **ìƒì„±ëª¨ë¸ (Generative Model)** ì´ë‹¤.  ê¸°ì¡´ì˜ AutoEncoderëŠ” latent vectorë¥¼ ë‹¨ìˆœí•œ ë²¡í„°ë¡œ ì••ì¶•í•˜ì§€ë§Œ, VAEëŠ” ì´ë¥¼ **í™•ë¥ ë¶„í¬**ë¡œ ë³¸ë‹¤.


![image](https://vitalflux.com/wp-content/uploads/2023/04/autoencoder-vs-variational-autoencoder-point-vs-distribution.png)



- ì•„ì´ë””ì–´: ë°ì´í„°ë¥¼ ì ì¬ ê³µê°„(z)ìœ¼ë¡œ ì¸ì½”ë”© í›„ **í™•ë¥ ì ìœ¼ë¡œ ë³µì›**
- ìˆ˜ì‹: 
$p(x, z) = p(x|z)p(z)$
- ëª©ì : ELBO (Evidence Lower Bound) ìµœì í™”
- **ì¥ì **: í•™ìŠµ ì•ˆì •, í™•ë¥  ê¸°ë°˜ ìƒì„±
- **ë‹¨ì **: ì´ë¯¸ì§€ê°€ íë¦¿í•  ìˆ˜ ìˆìŒ

---
*ì²˜ìŒ ê³µë¶€í•  ë•Œ 'Variational'ì´ë¼ëŠ” ë‹¨ì–´ê°€ ì‰½ê²Œ ì™€ë‹¿ì§€ ì•Šì•„ì„œ ê³ ìƒí–ˆì—ˆë˜ ëª¨ë¸ì´ì—ˆë‹¤..*

--- 


#### âšª ìˆ˜ì‹ ì •ë¦¬

VAEì˜ ëª©ì ì€ ë‹¤ìŒì˜ evidence lower bound (ELBO)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒ:

$$
\log p(x) \ge \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) \| p(z))
$$

- ì²« í•­: reconstruction term 
- ë‘ ë²ˆì§¸ í•­: latent ë¶„í¬ë¥¼ ì •ê·œ ë¶„í¬ì— ê°€ê¹ê²Œ ìœ ë„

---


#### âšª êµ¬ì¡° ìš”ì•½

![VAE](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2Fb30Uzl%2FbtrxY4wKngj%2FAAAAAAAAAAAAAAAAAAAAAJu4BtneRYDaEXhXydnfS-HDbwKgj1HdsraWKovzKtUz%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1756652399%26allow_ip%3D%26allow_referer%3D%26signature%3DGNMbQlKtefwYudXjs9gC9jMmK%252BI%253D)

- **Encoder**: ì…ë ¥ $$ x $$ â†’ latent ë¶„í¬ $$ z \sim \mathcal{N}(\mu, \sigma^2) $$
- **Decoder**: $$ z $$   â†’   ë³µì›ëœ ì´ë¯¸ì§€ $$ \hat{x} $$
- **Loss**:
  1. ë³µì› ì†ì‹¤: $$ \|x - \hat{x}\|^2 $$
  2. ë¶„í¬ regularization (KL divergence): $$ D_{KL}(q(z\|x) \| p(z)) $$



---

#### âšª íŠ¹ì§•

| í•­ëª©       | ì„¤ëª… |
|------------|------|
| ì¥ì        | Latent spaceê°€ ì—°ì†ì , sampling ê°€ëŠ¥ |
| ë‹¨ì        | ì´ë¯¸ì§€ í’ˆì§ˆì´ blurryí•¨ (pixel-wise loss ë•Œë¬¸) |
| ì‘ìš©       | Image generation, anomaly detection, disentanglement |



---

## ğŸŸ£ ë§ˆì¹˜ë©°
VAEì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ì¸ì½”ë”ê°€ ì´ë¯¸ì§€ë¥¼ ì ì¬ ê³µê°„ì˜ **íŠ¹ì • í•œ ì (a single point)** ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ëŒ€ì‹ , **í™•ë¥  ë¶„í¬(a probability distribution)** ë¡œ ë§¤í•‘í•˜ë„ë¡ í•´ì„œ latent spaceê°€ ë¶€ë“œëŸ½ê²Œ ì±„ì›Œì§€ë„ë¡ í–ˆë‹¤.

ì´ëŸ¬í•œ ì•„ì´ë””ì–´ëŠ” ë”¥ëŸ¬ë‹ ê³µë¶€ì—ì„œ generative modelì˜ ì´í•´ë¥¼ ë•ê³ , ë˜í•œ í†µê³„ì  ì—­ëŸ‰ì„ ëŠ˜ë¦´ ìˆ˜ ìˆëŠ” ìœ ì˜ë¯¸í•œ ê³µë¶€ì¼ ê²ƒì´ë‹¤.


---

## Reference
- [Kingma & Welling, â€œAuto-Encoding Variational Bayesâ€, ICLR, 2014](https://arxiv.org/abs/1312.6114)