---
layout: post
title:  "11-(1) BERT와 GPT의 주요 차이점은 무엇인가요? 각각의 기본 구조와 작동 방식, 적합한 NLP 응용 분야를 위주로 설명해주세요."
date:   2025-08-22 11:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [BERT, GPT, NLP, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)

---


## 🟢 TBU
---
#### ⚪ TBU..

## 🟢 예시 답안 (코드잇 제공)
>
BERT와 GPT는 모두 Transformer 구조를 기반으로 한 사전학습(pre-trained) 언어 모델이지만, 구조와 작동 방식, 그리고 주로 사용되는 NLP 응용 분야에서 몇 가지 중요한 차이점이 있습니다.<br><br>
**[구조와 작동 방식]**  
BERT는 Transformer의 인코더(Encoder)만 사용하는 구조입니다. 입력 문장을 양방향으로 동시에 바라보며 문맥을 이해하는 양방향(Bidirectional) 구조가 특징입니다. 이 모델은 문장 내에서 일부 단어를 가린 뒤(Masked Language Modeling), 그 가려진 단어를 예측하는 방식으로 사전학습이 진행됩니다. 또한 두 문장이 연결되는지를 예측하는 Next Sentence Prediction(NSP) 작업도 함께 수행합니다.  
반면 GPT는 Transformer의 디코더(Decoder)만 사용하는 구조로 구성되어 있습니다. GPT는 한 방향, 즉 왼쪽에서 오른쪽으로 순차적으로 단어를 예측하는 단방향(Unidirectional) 구조입니다. 이 모델은 다음에 올 단어를 예측하는 언어 모델링 방식(Autoregressive Language Modeling)으로 사전학습되며, 입력 문장의 앞부분만을 활용해 다음 단어를 생성합니다.<br><br> 
**[적합한 NLP 응용 분야]**  
BERT는 언어 이해(understanding) 중심의 작업에 적합합니다. 예를 들어 문서 분류, 문장 유사도 판단, 개체명 인식, 질의응답처럼 문장을 해석하거나 의미를 파악해야 하는 작업에서 좋은 성능을 보입니다.  
반면 GPT는 언어 생성(generation) 중심의 작업에 강합니다. 예를 들어 텍스트 자동 완성, 대화형 응답 생성, 요약, 창작형 글쓰기, 코드 생성 등 문장을 창의적으로 생성하는 작업에 잘 활용됩니다. <br><br>
**[정리]**  
BERT는 인코더 기반으로 '문장을 이해'하는 데 최적화된 모델이고, GPT는 디코더 기반으로 '문장을 생성'하는 데 강점을 가진 모델이라고 볼 수 있습니다.
