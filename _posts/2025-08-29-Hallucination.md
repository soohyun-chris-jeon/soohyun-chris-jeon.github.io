---
layout: post
title:  "12-(1) LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란 무엇이고, 왜 문제가 되나요?"
date:   2025-08-29 12:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [LLM, Hallucination, BERT, GPT, NLP, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)
image:
    path: https://yozm.wishket.com/media/news/2733/image8.png
---


## 🟢 LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란 무엇이고, 왜 문제가 되나요? 여러 LLM 서비스들은 할루시네이션 문제를 어떻게 극복하려고 시도 중일까요? 구글링 등을 통해 자유롭게 리서치해보세요.

개인적으로 이제 코딩을 하거나 어떤 일을 할때도 LLM이 없으면 일이 안되는 수준으로 LLM에 의존도는 점점 커지고 있다.

사용하고 있는 모델은 구글 Gemini pro와 챗GPT 무료 버전을 사용 중이다.

그러나 이렇게 LLM을 활용하다 보면 할루시네이션(Hallucination)이라는 용어를 심심찮게 듣기도 하고 실제로 LLM의 답변에서도 '왜 모르는데 아는 척을 하는거지?'라는 생각을 할 때가 종종 있었다. 이럴때면 LLM에 대한 의존도는 높아졌지만 마냥 믿고 쓰기는 어렵겠다는 생각도 자주 든다.

이러한 문제점이 바로 **할루시네이션(Hallucination, 환각)**이다. 

모델이 **사실이 아니거나, 주어진 맥락과 관련 없는 정보를 마치 사실인 것처럼 그럴듯하게 만들어내는 현상**을 말한다. 헛것을 본다는 원래 의미처럼, AI가 존재하지 않는 정보를 보고 이야기하는 것에 비유할 수 있다.

예를 들어, "세종대왕 맥북 던짐 사건"처럼 역사적으로 말이 안 되는 이야기를 실제 있었던 일처럼 묘사하거나, 세상에 존재하지 않는 논문이나 책을 진짜인 것처럼 인용하는 경우가 대표적인 할루시네이션이다.

앞으로 LLM 활용을 더 잘하기 위해서 이번 기회에 할루시네이션에 대해서 정리해본다.

---
#### ⚪ 왜 문제가 되나요?

LLM의 할루시네이션은 단순한 실수를 넘어 여러 가지 심각한 문제를 일으킬 수 있습니다.

##### **1. 잘못된 정보의 확산 (Misinformation)**

가장 큰 문제입니다. LLM은 문법적으로 매우 자연스럽고 논리적인 것처럼 보이는 문장을 생성하기 때문에, 사용자는 그 내용이 거짓이라는 사실을 알아채기 어렵습니다. 만약 사용자가 이 정보를 그대로 믿고 블로그나 소셜 미디어에 공유한다면, **가짜 뉴스가 걷잡을 수 없이 퍼져나가** 사회적 혼란을 야기할 수 있습니다.

##### **2. 신뢰도 저하 (Erosion of Trust)**

AI 시스템에 대한 사용자의 신뢰를 근본적으로 무너뜨립니다. 중요한 업무나 의사결정에 LLM을 활용했는데, 결정적인 순간에 할루시네이션으로 인해 잘못된 결과를 얻는다면 해당 기술 자체를 더 이상 신뢰하지 않게 됩니다. 특히 **의료, 법률, 금융과 같이 정확성이 생명인 분야**에서는 단 한 번의 할루시네네이션이 치명적인 결과를 초래할 수 있습니다.

##### **3. 잠재적 위험 및 책임 문제**

예를 들어, LLM이 잘못된 법률 정보를 제공하여 사용자가 법적 불이익을 받거나, 존재하지 않는 약품 정보를 생성하여 건강상의 위험을 초래하는 경우 그 책임 소재가 불분명해집니다. 이처럼 할루시네이션은 단순히 부정확한 정보를 넘어 **실질적인 피해**로 이어질 수 있는 위험성을 내포하고 있습니다.

![할루시네이션](https://cdn.hansbiz.co.kr/news/photo/202505/748480_742881_5536.jpg)

---

#### ⚪ 할루시네이션은 왜 발생할까요?

할루시네이션이 발생하는 이유는 복합적이지만, 주로 다음과 같은 원인 때문입니다.

* **학습 데이터의 한계**: 모델이 학습한 데이터에 오래된 정보, 편향된 내용, 혹은 사실이 아닌 정보가 포함되어 있을 경우, 이를 그대로 학습하여 잘못된 내용을 생성할 수 있습니다.
* **확률 기반 생성의 본질**: LLM은 문맥을 '이해'해서 답변하는 것이 아니라, 주어진 단어 다음에 올 가장 그럴듯한(확률적으로 높은) 단어를 예측하여 문장을 조합합니다. 이 과정에서 사실관계는 무시되고, 그럴듯함만 남는 경우가 생깁니다.
* **정보 부족**: 모델이 한 번도 학습한 적 없는 최신 정보나 매우 전문적인 분야에 대해 질문받으면, 아는 정보를 조합하여 추측성 답변을 만들어내려는 경향이 있습니다.

---


#### ⚪ 할루시네이션에 대한 최신 기술 레포트 정리

##### 1. 스탠포드 HAI: 할루시네이션의 원인과 결과에 대한 종합 보고서

스탠포드 인간 중심 AI 연구소(HAI)에서 발표한 **"On the Dangers of Stochastic Parrots"** 와 그 후속 연구들은 할루시네이션 문제에 대한 깊이 있는 분석을 제공합니다.

* **주요 내용**: 이들은 LLM을 앵무새처럼 **"의미를 이해하지 못한 채 통계적으로 그럴듯한 텍스트를 조합해 흉내 내는 존재(Stochastic Parrots)"**로 비유했습니다. 할루시네이션은 모델이 진짜 '이해'를 하는 것이 아니기 때문에 발생하는 본질적인 한계라는 거야.
* **흥미로운 점**: 단순히 기술적 결함을 넘어, 이런 할루시네이션이 만들어내는 잘못된 정보가 사회적 편견을 강화하거나, 가짜 뉴스를 확산시키는 등 **사회에 미치는 위험성**을 심도 있게 경고했다는 점에서 큰 반향을 일으켰습니다. 기술의 윤리적, 사회적 책임을 강조한 중요한 리포트입니다.



---

##### 2. "알면서도 거짓말한다?" - LLM의 자기 평가(Self-Correction) 연구

최근에는 "LLM이 자신의 답변이 할루시네이션인지 아닌지를 스스로 알고 있을까?"라는 흥미로운 질문에 대한 연구가 활발합니다.

* **주요 내용**: MIT 등의 연구에 따르면, LLM은 답변을 생성하기 전에 내부적으로 **자신의 답변에 대한 '확신도(confidence)'나 '불확실성(uncertainty)'을 측정**할 수 있다고 합니다. 특정 답변을 내놓을 때의 내부 확률 분포나 신경망 활성화 패턴을 분석하면, 모델이 확신을 갖고 답하는지 아니면 정보를 지어내고 있는지(할루시네이션)를 어느 정도 예측할 수 있다는 점.
* **흥미로운 점**: 이는 할루시네이션이 단순히 '지식이 없어서' 발생하는 게 아니라, 때로는 **'불확실한 상황에서 가장 그럴듯한 답변을 선택하는' 모델의 작동 방식**과 관련이 깊다는 걸 시사합니다. 앞으로는 모델이 "잘 모르겠습니다"라고 솔직하게 답하도록 유도하거나, 스스로 답변을 검증하고 수정하는 **자기 교정(Self-Correction)** 능력을 강화하는 방향으로 연구가 진행되고 있습니다.

---

##### 3. RAG: 할루시네이션의 가장 현실적인 해결책

**검색 증강 생성(Retrieval-Augmented Generation, RAG)**은 특정 모델의 이름이라기보다는 할루시네이션을 완화하기 위한 '프레임워크'로, 현재 가장 주목받고 있는 기술 리포트의 단골 주제이다.

* **주요 내용**: LLM이 내부 지식만으로 답변하게 두지 않고, 질문이 들어오면 먼저 **신뢰할 수 있는 외부 데이터베이스(DB)나 최신 문서를 검색(Retrieve)**하게 만든다. 그리고 검색된 **'사실 기반' 자료**를 근거로 답변을 생성(Generate)하도록 하는 기술.
* **흥미로운 점**: 이 방식은 LLM의 약점인 '사실성'은 외부 검색 엔진에 맡기고, LLM의 강점인 '자연스러운 문장 생성' 능력만 활용하는 영리한 접근법이다. 최신 정보를 반영하지 못하거나, 특정 분야의 전문 지식이 부족해 발생하는 할루시나이션을 극적으로 줄일 수 있어서 현재 가장 실용적이고 효과적인 해결책으로 평가받고 있습니다.



## 🟢 결론

결론적으로 할루시네이션은 LLM이 아직 세상을 진정으로 '이해'하지 못하고, 단지 데이터의 통계적 패턴을 흉내 내는 존재임을 보여주는 가장 큰 한계점 중 하나이며, 이 문제를 해결하는 것이 LLM 기술의 신뢰성과 안전성을 확보하는 핵심 과제라고 할 수 있습니다.


## 🟢 예시 답안 (코드잇 제공)
> - LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란, 모델이 실제로 존재하지 않는 정보나 사실과 다른 내용을 그럴듯하게 생성하는 현상을 말합니다. 예를 들어, 존재하지 않는 논문 제목을 만들어 내거나, 실제로는 관련 없는 사실을 연결해 설명하는 경우가 할루시네이션에 해당합니다.<br> 이 문제는 특히 신뢰성과 정확성이 중요한 분야에서 큰 문제가 됩니다. 예를 들어, 의료, 법률, 금융 같은 도메인에서는 모델이 잘못된 정보를 사실처럼 말할 경우 치명적인 의사결정 오류로 이어질 수 있습니다. 또한, 모델이 허구의 정보를 스스로 '믿는 듯한' 문장으로 생성하기 때문에 사용자가 쉽게 속을 수 있다는 점도 위험 요소입니다.<br><br> LLM 서비스들이 할루시네이션 문제를 극복하기 위해 시도 중인 주요 접근 방식은 다음과 같습니다.<br><br>
- Retrieval-Augmented Generation (RAG)  
외부 지식 베이스나 문서를 검색한 뒤, 그 검색 결과를 기반으로 응답을 생성하는 방식입니다.
이를 통해 모델은 내부의 불완전한 파라미터 지식에 의존하지 않고, 실시간 정보 기반으로 답변할 수 있게 됩니다.<br>
- Fact-checking 모델 또는 후처리 필터  
생성된 응답을 다시 검토하고, 신뢰도가 낮거나 사실과 불일치하는 문장을 제거하거나 수정하는 후처리 모델을 사용합니다.
- 지식 기반 모델 정렬 (Alignment with Knowledge Bases)  
LLM을 위키피디아, 논문 DB, 전문 도메인 지식 등 신뢰성 있는 데이터와 정렬시키는 방식입니다.
FLAN이나 InstructGPT 등도 사용자 의도에 맞는 사실 기반 응답을 유도하기 위해 학습 과정에서 지침(prompt instruction) 기반 학습을 적용합니다.<br>
- Human Feedback 기반 강화 학습 (RLHF)  
사람이 응답의 품질과 사실 여부를 평가한 피드백을 기반으로 모델을 조정합니다.
이 과정은 LLM이 인간의 기대와 기준에 더 잘 맞도록 응답을 정렬(alignment)시키는 데 중요한 역할을 합니다.<br>
- 출처 명시 및 인용 기능 추가  
최근 일부 LLM 서비스에서는 응답에 인용한 웹사이트나 문서 링크를 함께 제공하여, 사용자가 직접 사실 여부를 검증할 수 있도록 유도하고 있습니다.