---
layout: post
title:  "12-(1) LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란 무엇이고, 왜 문제가 되나요?"
date:   2025-08-29 12:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [LLM, Hallucination, BERT, GPT, NLP, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)

---


## 🟢 LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란 무엇이고, 왜 문제가 되나요? 여러 LLM 서비스들은 할루시네이션 문제를 어떻게 극복하려고 시도 중일까요? 구글링 등을 통해 자유롭게 리서치해보세요.

---
#### ⚪ TBU..

## 🟢 예시 답안 (코드잇 제공)
> - LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란, 모델이 실제로 존재하지 않는 정보나 사실과 다른 내용을 그럴듯하게 생성하는 현상을 말합니다. 예를 들어, 존재하지 않는 논문 제목을 만들어 내거나, 실제로는 관련 없는 사실을 연결해 설명하는 경우가 할루시네이션에 해당합니다.<br> 이 문제는 특히 신뢰성과 정확성이 중요한 분야에서 큰 문제가 됩니다. 예를 들어, 의료, 법률, 금융 같은 도메인에서는 모델이 잘못된 정보를 사실처럼 말할 경우 치명적인 의사결정 오류로 이어질 수 있습니다. 또한, 모델이 허구의 정보를 스스로 '믿는 듯한' 문장으로 생성하기 때문에 사용자가 쉽게 속을 수 있다는 점도 위험 요소입니다.<br><br> LLM 서비스들이 할루시네이션 문제를 극복하기 위해 시도 중인 주요 접근 방식은 다음과 같습니다.<br><br>
- Retrieval-Augmented Generation (RAG)  
외부 지식 베이스나 문서를 검색한 뒤, 그 검색 결과를 기반으로 응답을 생성하는 방식입니다.
이를 통해 모델은 내부의 불완전한 파라미터 지식에 의존하지 않고, 실시간 정보 기반으로 답변할 수 있게 됩니다.<br>
- Fact-checking 모델 또는 후처리 필터  
생성된 응답을 다시 검토하고, 신뢰도가 낮거나 사실과 불일치하는 문장을 제거하거나 수정하는 후처리 모델을 사용합니다.
- 지식 기반 모델 정렬 (Alignment with Knowledge Bases)  
LLM을 위키피디아, 논문 DB, 전문 도메인 지식 등 신뢰성 있는 데이터와 정렬시키는 방식입니다.
FLAN이나 InstructGPT 등도 사용자 의도에 맞는 사실 기반 응답을 유도하기 위해 학습 과정에서 지침(prompt instruction) 기반 학습을 적용합니다.<br>
- Human Feedback 기반 강화 학습 (RLHF)  
사람이 응답의 품질과 사실 여부를 평가한 피드백을 기반으로 모델을 조정합니다.
이 과정은 LLM이 인간의 기대와 기준에 더 잘 맞도록 응답을 정렬(alignment)시키는 데 중요한 역할을 합니다.<br>
- 출처 명시 및 인용 기능 추가  
최근 일부 LLM 서비스에서는 응답에 인용한 웹사이트나 문서 링크를 함께 제공하여, 사용자가 직접 사실 여부를 검증할 수 있도록 유도하고 있습니다.