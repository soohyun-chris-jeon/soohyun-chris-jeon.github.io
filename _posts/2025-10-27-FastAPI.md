---
layout: post
title:  "16-(2) FastAPIë¥¼ í™œìš©í•˜ì—¬ AI ëª¨ë¸ì„ í†µí•©í•œ ì›¹ APIë¥¼ êµ¬í˜„í•  ë•Œ, ê¸°ëŠ¥ë³„ë¡œ ë‚˜ëˆ ì„œ êµ¬ì„±í•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•©ë‹ˆë‹¤. ì „ì²´ì ì¸ API ì„œë²„ ì½”ë“œ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ êµ¬ì„±ë˜ë©´ ì¢‹ì„ì§€ ììœ ë¡­ê²Œ ì‘ì„±í•´ë³´ì„¸ìš”."
date:   2025-10-27 01:00:00 +0900
categories: [FastAPI]
tags: [FastAPI, Quantization, Docker, Deep Learning, AI]
comments: true     # ëŒ“ê¸€ ê¸°ëŠ¥ ì‚¬ìš© (ì˜µì…˜)
image:
    path: https://miro.medium.com/v2/resize:fit:1400/1*fNYR_qApiq8wPbDxQvar2Q.png

---
## ğŸŸ¢ Intro
Streamlitì´ í”„ë¡œí† íƒ€ì´í•‘ê³¼ ë°ëª¨ì— ê°•ì ì´ ìˆë‹¤ë©´, FastAPIëŠ” **ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ AI ëª¨ë¸ ì„œë¹™**ì— ì••ë„ì ìœ¼ë¡œ ìœ ë¦¬í•˜ë‹¤. 'ê¸°ëŠ¥ë³„ ë¶„ë¦¬' (ê´€ì‹¬ì‚¬ ë¶„ë¦¬, Separation of Concerns)ëŠ” ìœ ì§€ë³´ìˆ˜ì™€ í˜‘ì—…ì„ ìœ„í•´ í•„ìˆ˜ì ì´ì§€.

ëŒ€ê·œëª¨ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²¬ê³ í•œ ë””ë ‰í„°ë¦¬ êµ¬ì¡°ë¥¼ AI ëª¨ë¸ APIì— ë§ê²Œ ë³€í˜•í•´ì„œ ì œì•ˆí•´ ë³¼ê²Œ.



---

#### âšª  AI ëª¨ë¸ ì„œë¹™ì„ ìœ„í•œ FastAPI í”„ë¡œì íŠ¸ êµ¬ì¡° 

í•µì‹¬ì€ **ë¼ìš°í„°(API ê²½ë¡œ)**, **ìŠ¤í‚¤ë§ˆ(ë°ì´í„° ê²€ì¦)**, \*\*ì„œë¹„ìŠ¤(í•µì‹¬ ë¡œì§)\*\*ë¥¼ ë¶„ë¦¬í•˜ëŠ” ê±°ì•¼.

```bash
my_ai_api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py             # 1. FastAPI ì•± ìƒì„± ë° ì „ì²´ ì„¤ì •
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ router.py         # 2. API ë¼ìš°í„° (ì—”ë“œí¬ì¸íŠ¸)
â”‚   â”‚   â””â”€â”€ schemas.py        # 3. Pydantic ë°ì´í„° ìŠ¤í‚¤ë§ˆ (ê²€ì¦)
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ config.py         # 4. ì„¤ì • (í™˜ê²½ ë³€ìˆ˜, ëª¨ë¸ ê²½ë¡œ ë“±)
â”‚   â”‚
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ model_service.py  # 5. AI ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ë¡œì§ (í•µì‹¬)
â”‚
â”œâ”€â”€ models/                   # 6. í›ˆë ¨ëœ ëª¨ë¸ íŒŒì¼ (ì˜ˆ: .pth, .h5, .onnx)
â”‚   â””â”€â”€ best_model.pth
â”‚
â”œâ”€â”€ .env                      # 7. í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ (config.pyê°€ ì½ì–´ ê°)
â”œâ”€â”€ requirements.txt
â””â”€â”€ run.py                    # (ì„ íƒ) uvicorn ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
```

-----

## ğŸ“œ ê° íŒŒì¼ ë° ë””ë ‰í„°ë¦¬ì˜ ì—­í• 

ê° êµ¬ì„± ìš”ì†Œê°€ ì–´ë–¤ 'ê¸°ëŠ¥'ì„ ë‹´ë‹¹í•˜ëŠ”ì§€ ì„¤ëª…í•´ ì¤„ê²Œ.

### 1\. `app/main.py` : ì•±ì˜ ì§„ì…ì  (Conductor)

ê°€ì¥ ìƒìœ„ ë ˆë²¨ì˜ íŒŒì¼ì´ì•¼. FastAPI ì•± ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³ , ë¯¸ë“¤ì›¨ì–´(CORS ë“±)ë¥¼ ì„¤ì •í•˜ê³ , `api/router.py`ì— ì •ì˜ëœ ë¼ìš°í„°ë“¤ì„ 'í¬í•¨(`include_router`)'ì‹œì¼œ.

**ê°€ì¥ ì¤‘ìš”í•œ ì—­í• **: ì•±ì´ ì‹œì‘(startup)ë  ë•Œ AI ëª¨ë¸ì„ **ë‹¨ í•œ ë²ˆë§Œ** ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ë„ë¡ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ ì„¤ì •í•´.

```python
# app/main.py
from fastapi import FastAPI
from app.api.router import api_router
from app.services.model_service import load_model_on_startup, close_model_on_shutdown

app = FastAPI(title="My AI Model API")

# ì•± ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
@app.on_event("startup")
async def startup_event():
    load_model_on_startup()

# ì•± ì¢…ë£Œ ì‹œ (ì„ íƒ ì‚¬í•­)
@app.on_event("shutdown")
async def shutdown_event():
    close_model_on_shutdown()

# /api/v1 ê³¼ ê°™ì€ í”„ë¦¬í”½ìŠ¤ë¡œ ë¼ìš°í„° í¬í•¨
app.include_router(api_router, prefix="/api/v1")

@app.get("/")
def read_root():
    return {"message": "AI Model API is running!"}
```

### 2\. `app/api/router.py` : API ì—”ë“œí¬ì¸íŠ¸ (Controller)

ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ì ‘ì†í•˜ëŠ” URL ê²½ë¡œ (`/predict`, `/health_check` ë“±)ë¥¼ ì •ì˜í•´.

ì´ íŒŒì¼ì€ \*\*'ë¬´ì—‡ì„ ë°›ê³  ë¬´ì—‡ì„ ë°˜í™˜í• ì§€'\*\*ì—ë§Œ ì§‘ì¤‘í•´. ì‹¤ì œ ë³µì¡í•œ ë¡œì§ì€ `model_service.py`ì— ìœ„ì„í•´. FastAPIì˜ \*\*ì˜ì¡´ì„± ì£¼ì…(Dependency Injection)\*\*ì„ ì‚¬ìš©í•˜ë©´ ê¹”ë”í•´ì ¸.

```python
# app/api/router.py
from fastapi import APIRouter, Depends, UploadFile, File
from app.api.schemas import PredictionRequest, PredictionResponse
from app.services.model_service import get_model_service, ModelService

api_router = APIRouter()

@api_router.post("/predict", response_model=PredictionResponse)
async def predict(
    request_data: PredictionRequest,  # JSON ì…ë ¥ì„ ë°›ì„ ë•Œ
    # file: UploadFile = File(...),   # ì´ë¯¸ì§€ íŒŒì¼ ë“±ì„ ë°›ì„ ë•Œ
    model_service: ModelService = Depends(get_model_service) # ì„œë¹„ìŠ¤ ì£¼ì…
):
    """
    ëª¨ë¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    # ì‹¤ì œ ë¡œì§ì€ ì„œë¹„ìŠ¤ ë ˆì´ì–´ë¡œ ë„˜ê¹€
    prediction = model_service.predict(request_data.input_data)
    
    return PredictionResponse(prediction=prediction)

@api_router.get("/health")
def health_check():
    return {"status": "ok"}
```

### 3\. `app/api/schemas.py` : ë°ì´í„° ê³„ì•½ì„œ (Pydantic Models)

APIê°€ ë°›ì„ \*\*ì…ë ¥(Request)\*\*ê³¼ ë°˜í™˜í•  \*\*ì¶œë ¥(Response)\*\*ì˜ ë°ì´í„° í˜•ì‹(íƒ€ì…, í•„ìˆ˜ í•„ë“œ ë“±)ì„ Pydanticìœ¼ë¡œ ì—„ê²©í•˜ê²Œ ì •ì˜í•´.

ì´ê²Œ FastAPIì˜ í•µì‹¬ì´ì•¼. ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ìë™ìœ¼ë¡œ í•´ì£¼ê³ , API ë¬¸ì„œë¥¼ ê¸°ê°€ ë§‰íˆê²Œ ë§Œë“¤ì–´ ì¤˜.

```python
# app/api/schemas.py
from pydantic import BaseModel, Field
from typing import List, Any

# ì˜ˆì‹œ: í…ìŠ¤íŠ¸ ì…ë ¥ì„ ë°›ëŠ” ê²½ìš°
class PredictionRequest(BaseModel):
    input_data: str = Field(..., example="ì´ ëª¨ë¸ì€ ë¬´ì—‡ì„ ì˜ˆì¸¡í•˜ë‚˜ìš”?")

# ì˜ˆì‹œ: ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë°›ëŠ” ê²½ìš°
# class PredictionRequest(BaseModel):
#     image_path: str = Field(..., example="/path/to/image.jpg")

class PredictionResponse(BaseModel):
    prediction: Any # ëª¨ë¸ì˜ ì¶œë ¥ì— ë§ê²Œ (e.g., str, float, List[float])
```

### 4\. `app/services/model_service.py` : í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (The Brain)

**ëª¨ë“  AI ê´€ë ¨ ë¡œì§ì€ ì—¬ê¸°ì— ìˆì–´.** ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ì…ë ¥ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬(preprocessing)í•˜ê³ , `model.predict()`ë¥¼ í˜¸ì¶œí•˜ê³ , ê²°ê³¼ë¥¼ í›„ì²˜ë¦¬(postprocessing)í•˜ëŠ” ì‹¤ì œ ì‘ì—… ê³µê°„ì´ì•¼.

  * `main.py`ì˜ `startup` ì´ë²¤íŠ¸ê°€ `load_model_on_startup`ì„ í˜¸ì¶œí•´.
  * `router.py`ì˜ ì—”ë“œí¬ì¸íŠ¸ëŠ” `predict` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´.

<!-- end list -->

```python
# app/services/model_service.py
import torch
from your_model_arch import YourModel # ì‹¤ì œ ëª¨ë¸ ì•„í‚¤í…ì²˜
from app.core.config import settings # ì„¤ì • íŒŒì¼ì—ì„œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°

# ëª¨ë¸ì„ ë‹´ì„ ì „ì—­ ë³€ìˆ˜ (ë˜ëŠ” í´ë˜ìŠ¤ ì†ì„±)
# ì•±ì´ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ì´ ë³€ìˆ˜ì— ëª¨ë¸ì´ í• ë‹¹ë¨
model_instance = None

class ModelService:
    def __init__(self, model):
        if model is None:
            raise ValueError("Model is not loaded!")
        self.model = model

    def _preprocess(self, data):
        # (ì˜ˆì‹œ) í…ìŠ¤íŠ¸ í† í°í™” ë˜ëŠ” ì´ë¯¸ì§€ í…ì„œ ë³€í™˜
        # tensor = ...
        # return tensor
        return data # ì„ì‹œ

    def _postprocess(self, output):
        # (ì˜ˆì‹œ) í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë‚˜ í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
        # result = ...
        # return result
        return output # ì„ì‹œ

    def predict(self, input_data: str):
        # 1. ì „ì²˜ë¦¬
        processed_data = self._preprocess(input_data)
        
        # 2. ì¶”ë¡ 
        # with torch.no_grad():
        #     output = self.model(processed_data)
        
        # (ì„ì‹œ ì˜ˆì‹œ ë¡œì§)
        output = f"Processed: {processed_data}"

        # 3. í›„ì²˜ë¦¬
        prediction_result = self._postprocess(output)
        
        return prediction_result

def load_model_on_startup():
    """ì•± ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹"""
    global model_instance
    try:
        # settings.MODEL_PATHëŠ” .env íŒŒì¼ì—ì„œ ì½ì–´ ì˜´
        # model = YourModel()
        # model.load_state_dict(torch.load(settings.MODEL_PATH))
        # model.eval()
        
        # (ì„ì‹œ ì˜ˆì‹œ) ì‹¤ì œ ëª¨ë¸ ëŒ€ì‹  ë”ë¯¸ ê°ì²´ í• ë‹¹
        model_instance = "MyLoadedAIModel" # ì‹¤ì œë¡œëŠ” model ê°ì²´
        print("Model loaded successfully!")
        
    except Exception as e:
        print(f"Error loading model: {e}")
        model_instance = None

def close_model_on_shutdown():
    """(ì„ íƒ) ì•± ì¢…ë£Œ ì‹œ ëª¨ë¸ ê´€ë ¨ ë¦¬ì†ŒìŠ¤ í•´ì œ"""
    global model_instance
    model_instance = None
    print("Model resources released.")


def get_model_service() -> ModelService:
    """
    ë¼ìš°í„°ì—ì„œ í˜¸ì¶œí•  ì˜ì¡´ì„± ì£¼ì… í•¨ìˆ˜.
    ë¡œë“œëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ì— ì£¼ì…í•˜ì—¬ ë°˜í™˜.
    """
    if model_instance is None:
        raise HTTPException(status_code=503, detail="Model is not loaded or failed to load.")
    return ModelService(model=model_instance)
```

### 5\. `app/core/config.py` : ì„¤ì • ê´€ë¦¬

ëª¨ë¸ íŒŒì¼ì˜ ê²½ë¡œ, ë°ì´í„°ë² ì´ìŠ¤ ì£¼ì†Œ, API í‚¤ ë“± í•˜ë“œì½”ë”©í•˜ë©´ ì•ˆ ë˜ëŠ” ê°’ë“¤ì„ ê´€ë¦¬í•´. Pydanticì˜ `BaseSettings`ë¥¼ ì‚¬ìš©í•˜ë©´ `.env` íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì½ì–´ì™€ì„œ í¸í•´.

```python
# app/core/config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    MODEL_PATH: str = "models/best_model.pth"
    API_V1_STR: str = "/api/v1"

    class Config:
        env_file = ".env" # .env íŒŒì¼ì„ ì½ë„ë¡ ì„¤ì •

settings = Settings()
```

### 6\. `models/` ë””ë ‰í„°ë¦¬ ë° `.env` íŒŒì¼

  * **`models/`**: `git lfs`ë¡œ ê´€ë¦¬ë˜ê±°ë‚˜ CI/CD íŒŒì´í”„ë¼ì¸ì„ í†µí•´ S3 ê°™ì€ ê³³ì—ì„œ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼ í•˜ëŠ” ì‹¤ì œ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸(.pth ë“±)ë¥¼ ì €ì¥í•´.
  * **`.env`**: `config.py`ê°€ ì½ì–´ê°ˆ ë¯¼ê° ì •ë³´ë‚˜ ê²½ë¡œë¥¼ ì €ì¥. (ì´ íŒŒì¼ì€ `.gitignore`ì— ê¼­ ì¶”ê°€í•´ì•¼ í•´\!)

<!-- end list -->

```bash
# .env
MODEL_PATH="models/my_final_model_v1.2.pth"
```


---


## ğŸŸ¢ ì˜ˆì‹œ ë‹µì•ˆ (ì½”ë“œì‡ ì œê³µ)


>.