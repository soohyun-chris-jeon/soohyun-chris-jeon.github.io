---
layout: post
title:  "10-(2) FastText가 Word2Vec과 다른 점은 무엇이며, 어떤 장점이 있나요?"
date:   2025-08-08 12:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [NLP, Word2Vec,embedding, oov FastText, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)
image:
    path: https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRp5ur9ZnrJ-BCZSQzSJVsnO_QSjmOiGxoTaw&s

---


## 🟢 FastText가 Word2Vec과 다른 점은 무엇이며, 어떤 장점이 있나요?

Word2Vec은 간단히 말해 **'단어를 의미를 가진 숫자의 배열(벡터)로 바꾸는 방법'**이다. 

딥러닝 모델이 이해할 수 있도록 단어의 '의미'를 숫자로 번역하는 기술이라고 생각하면 된다.

---
#### ⚪ Word2Vec의 한계점

Word2Vec은 단어를 벡터 공간에 표현하는 '워드 임베딩'의 시대를 연 혁신적인 모델임. 하지만 Word2Vec에는 명확한 한계가 존재했음. 바로 **단어를 '통째로' 학습**한다는 점이다. 이로 인해 두 가지 주요한 문제가 발생했다.

1.  **Out-of-Vocabulary (OOV) 문제**: 학습 코퍼스에 등장하지 않은 단어는 벡터를 생성할 수 없음.
2.  **희귀 단어(Rare Words) 문제**: 등장 빈도가 낮은 단어는 충분히 학습되지 않아 벡터의 품질이 떨어짐.

2016년, Facebook AI Research(FAIR)는 이 문제들을 해결하기 위해 Word2Vec의 구조를 계승하면서도 핵심적인 아이디어를 추가한 **FastText**를 발표했다.

---
#### ⚪ FastText의 핵심 아이디어: Subword Information

FastText가 Word2Vec과 다른 가장 큰 차이점은 **단어를 '부분 단어(Subword)'의 총합으로 바라본다**는 것입니다.

-   **Word2Vec**: `apple` → `apple` (하나의 단위로 취급)
-   **FastText**: `apple` → `<ap`, `app`, `ppl`, `ple`, `le>` + `<apple>` (문자 n-gram들의 집합 + 전체 단어)

Word2Vec이 `apple`이라는 단어 자체의 벡터만 학습하는 반면, FastText는 단어를 구성하는 문자 n-gram(e.g., 3~6글자)들의 벡터를 각각 학습합니다. 그리고 최종적으로 이 부분 단어 벡터들을 모두 더하여 원본 단어의 벡터를 표현합니다.

## FastText의 장점

이러한 Subword 접근 방식은 Word2Vec의 한계를 효과적으로 극복합니다.

### 1. OOV 단어에 대한 강건한(Robust) 대처

FastText의 가장 강력한 장점입니다. 만약 학습 데이터에 없던 `applesauce`라는 단어가 등장해도, FastText는 이 단어를 `<ap`, `app`, `ppl`, ... `sau`, `auc`, `uce>` 와 같은 부분 단어들로 분해합니다. 그리고 기존에 다른 단어들을 통해 학습해 둔 이 부분 단어 벡터들을 조합하여 `applesauce`의 벡터를 추정해낼 수 있습니다.

이 덕분에 FastText는 **신조어, 오타, 희귀 단어 등 OOV 문제에 훨씬 유연하게 대처**할 수 있습니다.

### 2. 풍부한 형태학적(Morphological) 정보 학습

부분 단어를 학습한다는 것은 단어의 형태학적 특징을 자연스럽게 임베딩에 녹여낼 수 있다는 의미입니다. 예를 들어, `helpful`과 `helpless`는 접미사 `-ful`과 `-less` 때문에 의미가 반대입니다. FastText는 `ful`과 `less`라는 부분 단어들의 벡터를 학습함으로써 이러한 형태적 유사성과 차이점을 벡터 공간에 효과적으로 표현할 수 있습니다.

이는 특히 문법적 변화가 복잡한 언어(예: 독일어, 터키어, 한국어)에서 더 큰 장점으로 작용합니다.

## 결론

FastText는 Word2Vec의 아이디어를 기반으로, Subword 정보를 활용하여 OOV 문제와 희귀 단어 처리 문제를 해결한 진보된 모델입니다. 단어를 더 작은 단위로 분해해서 바라보는 이 단순하면서도 강력한 아이디어는 이후 BERT의 WordPiece, XLNet의 SentencePiece와 같은 Subword Tokenizer의 등장에도 큰 영향을 미쳤습니다.


## 🟢 예시 답안 (코드잇 제공)
> 
- FastText는 Word2Vec과 비슷한 방식으로 단어 임베딩을 학습하는 모델이지만, 단어를 더 작은 단위인 서브워드(Subword)로 분해해서 학습한다는 점에서 차이가 있습니다. 
- Word2Vec은 단어 전체를 하나의 단위로 보고 벡터를 학습합니다. 예를 들어 'apple'이라는 단어가 있다면, 이 단어 자체에 대한 벡터만 학습됩니다. 반면 FastText는 'app', 'ppl', 'ple' 같은 n-gram 단위의 조각으로 단어를 분해하고, 이 조각들의 임베딩을 평균내거나 합쳐서 단어 벡터를 구성합니다. 
- 이 방식의 장점은 특히 형태가 유사한 단어들 간의 관계를 잘 반영할 수 있다는 점입니다. 예를 들어 'run', 'running', 'runner'처럼 비슷한 형태를 가진 단어들은 공통된 서브워드를 공유하므로, FastText는 이들 간의 의미적 유사성을 자연스럽게 학습할 수 있습니다. 
- 또 하나의 큰 장점은 OOV(Out-of-Vocabulary) 문제를 완화할 수 있다는 점입니다. Word2Vec은 학습 데이터에 없는 단어는 임베딩할 수 없지만, FastText는 서브워드 단위로 처리하기 때문에 처음 보는 단어라도 그 조각들을 이용해 벡터를 생성할 수 있습니다. 
- 결과적으로 FastText는 희귀 단어, 신조어, 오타 등이 포함된 데이터셋에서도 더 강건한 성능을 보일 수 있고, 특히 형태소 기반 언어나 언어 자원이 부족한 상황에서도 유용하게 활용될 수 있는 모델입니다.