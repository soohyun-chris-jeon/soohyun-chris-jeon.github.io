---
layout: post
title:  "[Part 3] Study log 2025.08.15 ~ 10.09  "
date:   2025-08-14 10:00:00 +0900
categories: [Codeit AI 3ê¸°, Study log]
tags: [NLP, LLM, python, Deep Learning, PyTorch, numpy]
image:
  path: https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F4qgsr%2FbtsFEtondnt%2FXoFKqUvKEaFyQubZZyLIPk%2Fimg.png

comments: true     # ëŒ“ê¸€ ê¸°ëŠ¥ ì‚¬ìš© (ì˜µì…˜)

---


<!-- 
![ì½”ë“œì‡ ìŠ¤í”„ë¦°íŠ¸](https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F4qgsr%2FbtsFEtondnt%2FXoFKqUvKEaFyQubZZyLIPk%2Fimg.png) -->

# ğŸŸ£ 2025.08.14 ~ 2025.10.02ì˜ ê¸°ë¡ 

## 2025-08-18 Mon
#### âšª í† í°í™”(tokenization)
```py
from nltk.tokenize import word_tokenize
text = "Although it's not a happily-ever-after ending, it is very realistic."
tokenized_words = word_tokenize(text)

```

#### âšª ì •ì œ(cleaning)
- ë“±ì¥ ë¹ˆë„, ë‹¨ì–´ ê¸¸ì´, ë¶ˆìš©ì–´ ê¸°ì¤€ ë“±
```py
from collections import Counter

# ë“±ì¥ ë¹ˆë„ ê¸°ì¤€ ì •ì œ í•¨ìˆ˜
def clean_by_freq(tokenized_words, cut_off_count):
    # íŒŒì´ì¬ì˜ Counter ëª¨ë“ˆì„ í†µí•´ ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ì¹´ìš´íŠ¸í•˜ì—¬ ë‹¨ì–´ ì§‘í•© ìƒì„±
    vocab = Counter(tokenized_words)
    
    # ë¹ˆë„ìˆ˜ê°€ cut_off_count ì´í•˜ì¸ ë‹¨ì–´ set ì¶”ì¶œ
    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}
    
    # uncommon_wordsì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]

    return cleaned_words

# ë‹¨ì–´ ê¸¸ì´ ê¸°ì¤€ ì •ì œ í•¨ìˆ˜
def clean_by_len(tokenized_words, cut_off_length):
    # ê¸¸ì´ê°€ cut_off_length ì´í•˜ì¸ ë‹¨ì–´ ì œê±°
    cleaned_by_freq_len = []
    
    for word in tokenized_words:
        if len(word) > cut_off_length:
            cleaned_by_freq_len.append(word)

    return cleaned_by_freq_len
```
## 2025-08-20 Wed
#### âšª ë¶ˆìš©ì–´(Stopwords)
# ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜
```py
def clean_by_stopwords(tokenized_words, stop_words_set):
    cleaned_words = []
    
    for word in tokenized_words:
        if word not in stop_words_set:
            cleaned_words.append(word)
            
    return cleaned_words
```

#### âšª ì–´ê°„ ì¶”ì¶œ(Stemming)
```py
from nltk.stem import PorterStemmer

# í¬í„° ìŠ¤í…Œë¨¸ ì–´ê°„ ì¶”ì¶œ í•¨ìˆ˜
def stemming_by_porter(tokenized_words):
    porter_stemmer = PorterStemmer()
    porter_stemmed_words = []

    for word in tokenized_words:
        stem = porter_stemmer.stem(word)
        porter_stemmed_words.append(stem)

    return porter_stemmed_words
```

#### âšª í’ˆì‚¬ íƒœê¹…
```py
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

# í’ˆì‚¬ íƒœê¹… í•¨ìˆ˜
def pos_tagger(tokenized_sents):
    pos_tagged_words = []

    for sentence in tokenized_sents:
        # ë‹¨ì–´ í† í°í™”
        tokenized_words = word_tokenize(sentence)
    
        # í’ˆì‚¬ íƒœê¹…
        pos_tagged = pos_tag(tokenized_words)
        pos_tagged_words.extend(pos_tagged)
    
    return pos_tagged_words
```


## 2025-08-21 Thu
#### âšª ì™¸ë¶€ ëª¨ë“ˆì˜ ìˆ˜ì •ì´ ë  ë•Œ ì•Œì•„ì„œ ìë™ìœ¼ë¡œ ë°˜ì˜ì´ ë˜ëŠ” ìµìŠ¤í…ì…˜ 
```py
%load_ext autoreload
%autoreload 2
```

#### âšª lemmatizer í‘œì œí™”
```py
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
nltk.download('wordnet')
nltk.download('omw-1.4')

def penn_to_wn(tag):
    if tag.startswith('J'):
        return wn.ADJ
    elif tag.startswith('N'):
        return wn.NOUN
    elif tag.startswith('R'):
        return wn.ADV
    elif tag.startswith('V'):
        return wn.VERB

def words_lemmatizer(pos_tagged_words):
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = []

    for word, tag in pos_tagged_words:
        wn_tag = penn_to_wn(tag)

        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):
            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag))
        else:
            lemmatized_words.append(word)

    return lemmatized_words
```


## 2025-08-25 Mon
#### âšª News Group 20 ë°ì´í„°: 18,846ê°œì˜ ë‰´ìŠ¤ ë¬¸ì„œë¥¼ 20ê°œì˜ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜
```py
from sklearn.datasets import fetch_20newsgroups
```

## 2025-08-26 Tue
#### âšª í´ëŸ¬ìŠ¤í„°ë§
- KMeans
- Elbow Method



## 2025-09-11 Fri
#### âšª ì„±ì·¨ë„ í‰ê°€
- [AI - LLM] RAG ë°©ì‹ì€ ê¸°ì¡´ì˜ ì‚¬ì „í•™ìŠµ ì–¸ì–´ ëª¨ë¸ê³¼ ì–´ë–¤ ì ì—ì„œ ë‹¤ë¥´ê²Œ ì‘ë™í•˜ë©°, ê·¸ë¡œ ì¸í•´ ì–´ë–¤ ì¥ì ì„ ê°–ëŠ”ì§€ 2ê°€ì§€ ì´ìƒ ì„œìˆ í•˜ì„¸ìš”. (20ì )

- [AI - LLM] RAGì˜ ì‘ë™ ì ˆì°¨ë¥¼ ë„¤ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•˜ì„¸ìš”. (20ì )
- [AI - LLM] í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²• ì¤‘ Chain of Thoughtì˜ ê°œë…, ì¥ì ê³¼ ë‹¨ì ì„ ì„œìˆ í•˜ì„¸ìš”. (20ì )
- [AI - LLM] PEFTì˜ ê°œë…ì„ ì„¤ëª…í•˜ê³ , ì¼ë°˜ì ì¸ ì „ì²´ íŒŒì¸íŠœë‹ê³¼ì˜ ì°¨ì´ì ì„ ì„œìˆ í•˜ì„¸ìš”. (20ì )
- [AI - LLM] BERTì™€ GPTì˜ ê³µí†µì ì„ ì„¤ëª…í•˜ì„¸ìš”. ê·¸ë¦¬ê³  ë‘ ëª¨ë¸ì˜ ì°¨ì´ì ì„ êµ¬ì¡°, í•™ìŠµ ë°©ì‹, í™œìš© ì¸¡ë©´ì—ì„œ ë¹„êµí•˜ì—¬ ì„œìˆ í•˜ì„¸ìš”. (20ì )
