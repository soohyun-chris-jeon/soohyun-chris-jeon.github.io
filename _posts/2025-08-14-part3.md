---
layout: post
title:  "[Part 2] Study log 2025.06.09 ~ 08.14  "
date:   2025-06-09 10:00:00 +0900
categories: [Codeit AI 3ê¸°, Study log]
tags: [python, Deep Learning, PyTorch, numpy]
comments: true     # ëŒ“ê¸€ ê¸°ëŠ¥ ì‚¬ìš© (ì˜µì…˜)

---



![ì½”ë“œì‡ ìŠ¤í”„ë¦°íŠ¸](https://img1.daumcdn.net/thumb/R750x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F4qgsr%2FbtsFEtondnt%2FXoFKqUvKEaFyQubZZyLIPk%2Fimg.png)

# ğŸŸ£ 2025.08.14 ~ 2025.10.02ì˜ ê¸°ë¡ 

## 2025-08-18 Mon
#### âšª í† í°í™”(tokenization)
```py
from nltk.tokenize import word_tokenize
text = "Although it's not a happily-ever-after ending, it is very realistic."
tokenized_words = word_tokenize(text)

```

#### âšª ì •ì œ(cleaning)
- ë“±ì¥ ë¹ˆë„, ë‹¨ì–´ ê¸¸ì´, ë¶ˆìš©ì–´ ê¸°ì¤€ ë“±
```py
from collections import Counter

# ë“±ì¥ ë¹ˆë„ ê¸°ì¤€ ì •ì œ í•¨ìˆ˜
def clean_by_freq(tokenized_words, cut_off_count):
    # íŒŒì´ì¬ì˜ Counter ëª¨ë“ˆì„ í†µí•´ ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ì¹´ìš´íŠ¸í•˜ì—¬ ë‹¨ì–´ ì§‘í•© ìƒì„±
    vocab = Counter(tokenized_words)
    
    # ë¹ˆë„ìˆ˜ê°€ cut_off_count ì´í•˜ì¸ ë‹¨ì–´ set ì¶”ì¶œ
    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}
    
    # uncommon_wordsì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]

    return cleaned_words

# ë‹¨ì–´ ê¸¸ì´ ê¸°ì¤€ ì •ì œ í•¨ìˆ˜
def clean_by_len(tokenized_words, cut_off_length):
    # ê¸¸ì´ê°€ cut_off_length ì´í•˜ì¸ ë‹¨ì–´ ì œê±°
    cleaned_by_freq_len = []
    
    for word in tokenized_words:
        if len(word) > cut_off_length:
            cleaned_by_freq_len.append(word)

    return cleaned_by_freq_len
```

