---
layout: post
title:  "MIT 6.S191: Large Language Models (Google) ê°•ì˜"
date:   2025-08-11 14:00:00 +0900
categories: [Deep Learning, LLM]
tags: [LLM, MIT, Google, Agent, Prompt Engineering]
image:
    path: https://i.ytimg.com/vi/ZNodOsz94cc/hq720.jpg?sqp=-oaymwEhCK4FEIIDSFryq4qpAxMIARUAAAAAGAElAADIQj0AgKJD&rs=AOn4CLARjVg-L2P4fLpHxDm_LDTU1yWtQg
description: "ğŸŸ£ êµ¬ê¸€ Gemini Applied Research groupì˜ Peter Grabowskiì˜ MIT ê°•ì—°"

---



# MIT 6.S191: Large Language Models

MITì˜ Deep Learning ê°•ì˜ ì‹œë¦¬ì¦ˆ ì¤‘ Googleì˜ **Alexander Amini**ê°€ ì§„í–‰í•œ LLM ì„¸ì…˜ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. LLMì˜ ê¸°ë³¸ ë™ì‘ ì›ë¦¬ë¶€í„° ìµœì‹  íŠ¸ë Œë“œì¸ Agentê¹Œì§€ ë‹¤ë£¨ê³  ìˆì–´ ì—°êµ¬ íë¦„ì„ ì¡ê¸°ì— ì¢‹ì€ ê°•ì˜ì…ë‹ˆë‹¤.

## 1. What are Large Language Models?

LLMì€ ë³¸ì§ˆì ìœ¼ë¡œ **Next Token Prediction**ì„ ìˆ˜í–‰í•˜ëŠ” í™•ë¥  ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë¥¼ **Auto-regressive decoding**ì´ë¼ê³  í•©ë‹ˆë‹¤.
ê°•ì˜ì—ì„œëŠ” ì´ë¥¼ "Fancy Autocomplete"ë¼ê³  í‘œí˜„í•˜ë©°, ë‹¨ìˆœí•œ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë„˜ì–´ ë¬¸ì œ í•´ê²° ë„êµ¬ë¡œì„œì˜ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

> "Language models are like fuzzy lookups back into their training data."

## 2. Key Drivers of Progress

ìµœê·¼ LLMì˜ ë¹„ì•½ì ì¸ ë°œì „ì€ ë‹¤ìŒ ìš”ì†Œë“¤ì— ê¸°ì¸í•©ë‹ˆë‹¤:

1.  **Scale (Parameters)**: ìˆ˜ì¡°(Trillions) ë‹¨ìœ„ì˜ íŒŒë¼ë¯¸í„° í™•ì¥. (BERT Largeê°€ 340Mì´ì—ˆë˜ ê²ƒì— ë¹„í•´ ì—„ì²­ë‚œ ì¦ê°€)
2.  **Data & Context Window**: Gemini ë“± ìµœì‹  ëª¨ë¸ì€ ìˆ˜ë°±ë§Œ í† í°ì˜ Contextë¥¼ ì²˜ë¦¬ ê°€ëŠ¥.
3.  **Emergent Behaviors**: ì¼ì • ê·œëª¨ ì´ìƒì—ì„œ **Zero-shot**, **Few-shot** ëŠ¥ë ¥ì´ ë°œí˜„ë¨.
    * *Reference: "Language Models are Few-Shot Learners" (NeurIPS 2020)*

## 3. Improving LLMs: Beyond Pre-training

Base Modelì„ ìœ ìš©í•œ Assistantë¡œ ë§Œë“¤ê¸° ìœ„í•œ ê¸°ë²•ë“¤ì…ë‹ˆë‹¤.

### Prompt Engineering
* **Role Prompting**: ëª¨ë¸ì—ê²Œ íŠ¹ì • í˜ë¥´ì†Œë‚˜(ì˜ˆ: MIT Mathematician)ë¥¼ ë¶€ì—¬í•˜ì—¬ ì •ë‹µë¥  í–¥ìƒ.
* **Chain of Thought (CoT)**: "Let's think step by step" í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ì¶”ë¡  ê³¼ì • ìœ ë„.

### Fine-tuning & Alignment
* **Instruction Tuning**: ì§€ì‹œì‚¬í•­(Instruction)ì„ ë”°ë¥´ë„ë¡ íŠœë‹.
* **RLHF**: ì¸ê°„ ì„ í˜¸ë„ë¥¼ ë°˜ì˜í•œ ê°•í™”í•™ìŠµ.
* **Constitutional AI**: Anthropicì—ì„œ ì œì•ˆí•œ ë°©ì‹ìœ¼ë¡œ, ì¸ê°„ í”¼ë“œë°± ëŒ€ì‹  ê·œì¹™(Constitution)ì„ ê¸°ë°˜ìœ¼ë¡œ AIê°€ AIë¥¼ í‰ê°€.
* **PEFT (Parameter-Efficient Fine-Tuning)**:
    * **LoRA**: ì „ì²´ íŒŒë¼ë¯¸í„° ëŒ€ì‹  Low-rank í–‰ë ¬ë§Œ í•™ìŠµí•˜ì—¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”. í•˜ë‚˜ì˜ Base Model ìœ„ì— ì—¬ëŸ¬ LoRA ì–´ëŒ‘í„°ë¥¼ ë¼ì›Œ ì“¸ ìˆ˜ ìˆìŒ.

## 4. Risks and Considerations

* **Hallucination**: ê·¸ëŸ´ë“¯í•˜ì§€ë§Œ í‹€ë¦° ì •ë³´ ìƒì„±. -> **RAG (Retrieval Augmented Generation)**ë¡œ ì™„í™”.
* **Bias**: í•™ìŠµ ë°ì´í„°ì˜ í¸í–¥ ë°˜ì˜.
* **Adversarial Attacks**: Jailbreaking, Prompt Injection.

## 5. AI Agents

LLMì´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ìƒì„±ì„ ë„˜ì–´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê³  ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ë‹¨ê³„ë¡œ ì§„í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.

* **ReAct (Reasoning + Acting)**:
    * ëª¨ë¸ì´ ì¶”ë¡ (Reasoning)ê³¼ í–‰ë™(Action)ì„ êµì°¨í•˜ë©° ìˆ˜í–‰.
    * *Reference: "ReAct: Synergizing Reasoning and Acting in Language Models"*
* **Toolformer**:
    * LLMì´ ìŠ¤ìŠ¤ë¡œ ì™¸ë¶€ API(ê³„ì‚°ê¸°, ê²€ìƒ‰ì—”ì§„ ë“±)ë¥¼ í˜¸ì¶œí•˜ëŠ” ë²•ì„ í•™ìŠµ.
    * API í˜¸ì¶œì´ Lossë¥¼ ì¤„ì´ëŠ”ì§€ íŒë‹¨í•˜ì—¬ ìœ ìš©í•œ í˜¸ì¶œë§Œ í•™ìŠµ ë°ì´í„°ë¡œ í•„í„°ë§.
    * *Reference: "Toolformer: Language Models Can Teach Themselves to Use Tools"*

