---
layout: post
title:  "12-(2) 모델 크기를 키우는 것만으로는 성능이 일정 시점 이후 둔화되는 이유는 무엇일까요?"
date:   2025-08-29 13:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [BERT, GPT, NLP, LLM, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)

---


## 🟢 TBU
---
#### ⚪ TBU..

## 🟢 예시 답안 (코드잇 제공)
> 
- 모델 크기를 키우면 초기에는 성능이 눈에 띄게 좋아지지만, 일정 규모를 넘어서면 성능 향상 속도가 점점 느려지고 결국에는 거의 개선되지 않는 구간에 도달하게 됩니다. 이 현상에는 몇 가지 이유가 있습니다. 
- 첫째, 학습 데이터의 한계입니다. 모델이 아무리 크더라도 학습할 수 있는 데이터가 부족하거나 품질이 낮으면 그 성능은 금방 한계에 부딪힙니다. 특히 데이터에 중복이 많거나 편향된 정보가 많으면, 큰 모델일수록 오히려 그 편향을 더 강하게 반영할 수 있습니다. 
- 둘째, 모델이 학습한 정보 중에는 실제로 문제 해결에 도움이 되지 않는 정보도 포함되기 때문에, 모델이 커질수록 반드시 '좋은 정보만 더 많이' 배우는 것은 아닙니다. 오히려 불필요한 패턴이나 잡음을 과도하게 학습할 가능성도 있습니다. 
- 셋째, 하드웨어 자원과 학습 비용의 문제도 있습니다. 모델이 커질수록 연산량과 메모리 사용량이 급격히 증가하므로, 학습 효율은 떨어지고 실제로 그만큼의 성능 향상을 얻기 어려워집니다. 즉, 비용 대비 성능 개선이 비효율적인 구간에 들어서는 거죠. 
결국 성능을 계속 높이기 위해서는 단순히 모델 크기를 키우는 것만으로는 부족하고, 더 나은 데이터, 정렬 기법(Model Alignment), 지식 보강, 또는 프롬프트 설계와 같은 다양한 기술이 함께 사용되어야 합니다.