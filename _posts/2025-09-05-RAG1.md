---
layout: post
title:  "13-(2) RAG 시스템의 성능을 평가하는 방법에는 어떤 것들이 있고, 독립 평가와 종단간 평가의 차이는 무엇인가요?"
date:   2025-09-05 11:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [RAG, LangChain, PEFT, BERT, GPT, NLP, LLM, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)

---


## 🟢 RAG 시스템의 성능을 평가하는 방법에는 어떤 것들이 있고, 독립 평가와 종단간 평가의 차이는 무엇인가요?

RAG 시스템의 성능 평가는 크게 **독립 평가(Component-wise Evaluation)**와 **종단간 평가(End-to-end Evaluation)** 두 가지 방식으로 나뉜다. 

* **독립 평가**는 팀원 개개인의 역할을 평가하는 것입니다. "자료 조사 담당(Retriever)은 관련 자료를 잘 찾아왔는가?", "PPT 제작 담당(Generator)은 그 자료로 발표 자료를 잘 만들었는가?"를 따로따로 점검하는 것이죠. **시스템의 어느 부분에서 문제가 발생하는지 진단(debugging)**할 때 유용합니다.

* **종단간 평가**는 팀의 최종 결과물, 즉 발표 전체를 평가하는 것입니다. 과정이 어찌 됐든 "그래서 최종 발표(Final Answer)가 청중을 만족시켰는가?"라는 **궁극적인 목표 달성 여부**를 확인하는 것이라고 생각할 수 있습니다.

---

#### ⚪ 주요 RAG 평가 방법

##### 1. **Retrieval 성능 평가 (독립 평가)**

retriever가 얼마나 "좋은 문서"를 잘 찾아주는지를 평가하는 단계.

* **정량적 지표 (Information Retrieval 지표)**

  * **Recall\@k**: 관련 문서가 검색된 상위 k개 안에 포함되는 비율
  * **Precision\@k**: 검색된 상위 k개 문서 중 실제로 관련 문서인 비율
  * **MRR (Mean Reciprocal Rank)**: 첫 번째 관련 문서가 몇 번째에 등장하는지 평가
  * **nDCG (normalized Discounted Cumulative Gain)**: 문서의 순위까지 고려한 품질 측정

👉 이 평가는 "검색 단계가 좋은 후보를 잘 뽑아주고 있는가?"를 보는 거라서 **retriever만 독립적으로** 테스트 가능.

---

##### 2. **Generation 성능 평가 (독립 평가)**

LLM이 주어진 context(검색된 문서)로 얼마나 좋은 답변을 생성하는지를 측정.

* **정량적 지표**

  * **BLEU / ROUGE / METEOR**: reference 답변과의 텍스트 유사도
  * **BERTScore**: 임베딩 기반 의미 유사도
  * **FactScore / Knowledge F1**: 답변이 실제 문서와 factually 일치하는지 평가
* **정성적 평가**

  * 인간 평가자(Human Evaluation) → 정확성(accuracy), 유창성(fluency), 관련성(relevance), 일관성(consistency) 점수

👉 여기서는 retriever 성능이 아니라 **LLM이 context를 얼마나 잘 활용했는지**가 포인트.


---

##### 3. **End-to-End 성능 평가 (종단간 평가)**

retrieval + generation을 합쳐서 최종 사용자 경험을 평가하는 단계.

* **정량적 지표**

  * **Exact Match (EM)**: 답변이 정답과 완전히 동일한 경우 비율
  * **QA F1 Score**: 답변과 정답 간 overlap 측정
  * **Faithfulness Score**: hallucination 없이 제공 문서에 기반했는지
* **정성적 평가**

  * 사용자 만족도 조사, human annotation 기반 채점

👉 이 평가는 실제로 사용자가 보는 “최종 답변” 기준이기 때문에, RAG 시스템을 배포하기 전에 가장 중요한 단계.

---

#### ⚪ 정리: 독립 평가 vs 종단간 평가

| 구분 | 독립 평가 (Component-level)            | 종단간 평가 (End-to-end)                 |
| -- | ---------------------------------- | ----------------------------------- |
| 초점 | Retrieval, Generation 각각 따로        | Retrieval + Generation 전체           |
| 장점 | 병목 구간(debugging point)을 정확히 파악     | 사용자 경험과 가장 가까움                      |
| 지표 | Recall\@k, MRR, ROUGE, BERTScore 등 | EM, QA F1, Faithfulness, Human Eval |
| 목적 | 어떤 단계에서 성능이 떨어지는지 분석               | 최종 서비스 품질 보장                        |


정리하면, **독립 평가는 디버깅/분석용**, **종단간 평가는 사용자 경험 평가용**이라고 볼 수 있다.


## 🟢 예시 답안 (코드잇 제공)


>RAG 시스템의 성능을 평가할 때는 검색(Retrieval)과 생성(Generation)이라는 두 가지 주요 단계에 대해 각각 또는 전체적으로 성능을 측정해야 합니다. 이때 평가 방식은 크게 독립 평가(모듈별 평가)와 종단간 평가(End-to-End 평가)로 나눌 수 있습니다.<br><br>먼저 독립 평가는 각 구성 요소의 성능을 분리해서 개별적으로 평가하는 방식입니다. 예를 들어 검색기의 성능은 주어진 질문에 대해 얼마나 관련 있는 문서를 찾아냈는지를 기준으로 평가하며, 대표적으로 Precision@k, Recall@k, MRR(Mean Reciprocal Rank) 같은 검색 기반 지표가 사용됩니다. 생성기 부분은 입력된 문서 기반으로 모델이 얼마나 자연스럽고 정확한 응답을 생성했는지, BLEU, ROUGE, METEOR, BERTScore 등의 자연어 생성 품질 지표로 측정할 수 있습니다.<br><br>반면 종단간 평가는 검색과 생성 과정을 통합해 전체 시스템이 사용자 질의에 대해 얼마나 잘 응답하는지를 측정하는 방식입니다. 여기서는 정답 일치율(Exact Match), 응답의 유용성, 사실성, 일관성 등을 평가하게 되며, 사람 평가(Human Evaluation)를 함께 병행하는 경우도 많습니다. 또한 최근에는 Faithfulness Score, Groundedness Score 같은 RAG 특화 평가 기준도 사용됩니다.<br><br>독립 평가는 시스템 내부의 병목이나 약점을 파악하는 데 유용하고, 종단간 평가는 사용자의 실제 경험에 더 가까운 평가 방식입니다. 두 방식은 서로 보완적이며, RAG 시스템을 고도화하기 위해서는 둘 다 적절히 활용하는 것이 중요합니다.