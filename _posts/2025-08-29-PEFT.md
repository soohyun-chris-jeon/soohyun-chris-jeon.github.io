---
layout: post
title:  "12-(3) PEFT가 필요한 이유는 무엇이며, 어떤 상황에서 특히 효과적인가요?"
date:   2025-08-29 14:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [PEFT, BERT, GPT, NLP, LLM, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)

---


## 🟢 TBU
---
#### ⚪ TBU..

## 🟢 예시 답안 (코드잇 제공)
> PEFT가 필요한 이유는 대형 언어 모델을 실제 환경에 맞게 활용하려고 할 때, 기존의 전체 파인튜닝 방식이 너무 비효율적이고 부담이 크기 때문입니다.<br>기존 방식은 모델의 모든 파라미터를 다시 학습해야 하다 보니, 수많은 GPU 자원이 필요하고, 학습 시간도 오래 걸리며, 각각의 작업마다 전체 모델을 따로 저장해야 해서 저장 공간도 많이 차지합니다. 특히 사전학습 모델의 크기가 수십억에서 수천억 파라미터로 커진 요즘에는, 이 방식이 현실적으로 어렵습니다.<br>이럴 때 PEFT는 전체 모델을 그대로 두고, 일부 파라미터만 학습하거나 작은 모듈만 추가로 학습하는 방식이기 때문에 훨씬 가볍고 빠릅니다. 전체 모델의 1%도 안 되는 파라미터만 수정하면 되니까, 자원이 적은 환경에서도 쉽게 사용할 수 있습니다.<br><br>PEFT가 특히 효과적인 상황은 다음과 같습니다.<br><br>첫째, 하나의 모델을 다양한 작업에 재활용해야 할 때입니다. 예를 들어 챗봇, 문서 분류, 질의응답 같은 여러 태스크를 동시에 다뤄야 하는 경우, 각각을 위해 전체 모델을 다시 학습하는 것보다, 작은 파인튜닝 모듈만 따로 관리하는 방식이 훨씬 효율적입니다.<br>둘째, 빠르게 실험하고 반복해야 하는 환경에서도 유리합니다. 전체 모델을 학습하는 데 며칠씩 걸리는 대신, PEFT는 몇 시간 내에 끝날 수 있어 연구 속도와 반복 실험 속도를 크게 높일 수 있습니다.<br>셋째, 모델을 클라우드가 아닌 로컬, 모바일, 엣지 환경에서 돌려야 할 때도 좋습니다. 작은 모듈만 추가하면 되기 때문에, 저장 용량도 적고 실행 속도도 빠르기 때문입니다.<br><br>결국 PEFT는 자원을 절약하면서도 높은 성능을 유지하고, 다양한 환경에서 유연하게 모델을 활용할 수 있게 해주는 현실적인 해결책입니다.