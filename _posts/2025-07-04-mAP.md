---
layout: post
title:  "8-(2) mAP(mean Average Precision)의 개념이 무엇이며 객체 인식에서 어떻게 활용되는지 설명해주세요."
date:   2025-07-04 11:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [mAP, PyTorch, CNN, convolutional-neural-network, Deep Learning, AI, Computer Vision]
comments: true     # 댓글 기능 사용 (옵션)

---


### 🟢 mAP(mean Average Precision)이란
---
#### ⚪ 모델의 성적표: mAP (mean Average Precision)

> Object Detection에서 표준⭐처럼 사용되는 지표


#### (1) IoU (Intersection over Union)

모델이 예측한 Bounding Box와 실제 정답 Bounding Box가 얼마나 잘 겹치는지를 0과 1 사이의 값으로 나타냅니다.

-   **IoU = (두 박스의 교집합 영역) / (두 박스의 합집합 영역)**

<img src="https://velog.velcdn.com/images/kwjinwoo/post/5ad12af2-e4bb-4971-aa81-ca5d24559414/image.png" width="500"  alt="IOU" >


보통 IoU가 특정 임계값(Threshold), 예를 들어 0.5 이상이면 모델의 예측이 '맞았다'(True Positive)고 판단.

#### (2) Precision (정밀도) vs. Recall (재현율)

-   **`Precision`**: 모델이 "객체다!"라고 예측한 것들 중, **실제로 객체인 비율** (FP, 즉 False Positive를 줄이는 것이 목표)
-   **`Recall`**: 실제 존재하는 모든 객체들 중, **모델이 얼마나 많이 찾아냈는지**의 비율 (FN, 즉 False Negative를 줄이는 것이 목표)

<img src="https://miro.medium.com/v2/resize:fit:970/1*XbE6Fx9P9Q0w5QG-52BaRQ.png" width="500"  alt="precision" />



일반적으로 두 지표는 반비례 관계(trade-off)에 있습니다. 모델의 예측 기준을 낮추면 더 많은 객체를 찾아내어 Recall은 올라가지만, 엉뚱한 것을 객체로 예측하는 경우가 많아져 Precision은 떨어짐

#### (3) Precision-Recall Curve와 AP (Average Precision)

모델의 신뢰도 임계값(Confidence Threshold)을 1부터 0까지 점차 낮춰가면서 각 지점에서의 Precision과 Recall 값을 계산하여 그래프로 그린 것이 **Precision-Recall Curve**

<img src="https://wiki.cloudfactory.com/media/pages/docs/mp-wiki/metrics/precision-recall-curve-and-auc-pr/2f7368bffa-1684131968/best-precision-recall-curves.webp" width="500"  alt="RP">

**AP(Average Precision)** 는 이 그래프의 아래쪽 면적(Area Under Curve, AUC)을 계산한 값. 즉, Precision과 Recall의 균형을 종합적으로 고려하여 단일 클래스에 대한 모델의 성능을 하나의 숫자로 나타낸 것. AP가 높을수록 해당 클래스에 대해 모델의 성능이 좋다고 말할 수 있음

#### (4) mAP (mean Average Precision)

**mAP**는 **모든 클래스에 대해 각각 AP를 계산한 뒤, 이를 평균**낸 값.

-   **mAP = (모든 클래스의 AP 합) / (총 클래스 수)**


---
#### ⚪ 결론적으로 

- **mAP**은 객관적으로 많은 Object detection 연구에서 표준이 되는 평가 지표로 활용되어 왔음.

- 앞으로 Object Detection 관련 논문이나 프로젝트를 접할 때, YOLO의 구조적 특징과 mAP라는 평가 지표를 알고 본다면 그 내용을 훨씬 더 빠르고 깊이 있게 이해할 수 있지 않을까

---

### 🟢 예시 답안 (코드잇 제공)
>  - mAP(mean Average Precision)는 객체 인식 모델의 성능을 평가하는 대표적인 지표로, 모델이 얼마나 정확하게 객체를 탐지하는지를 정량적으로 측정하는 데 사용됩니다.
- mAP는 먼저 AP(Average Precision)를 계산한 후, 모든 클래스에 대해 평균을 구하는 방식으로 정의됩니다. AP는 특정 클래스에 대해 Precision-Recall 곡선(Precision-Recall Curve)의 아래 영역(AUC, Area Under Curve)을 계산한 값입니다. 즉, Precision과 Recall의 균형을 반영하여 한 개의 숫자로 성능을 표현하는 지표입니다.
- 객체 인식에서는 예측된 바운딩 박스가 정답과 얼마나 잘 맞는지를 평가하기 위해 IoU(Intersection over Union)을 사용합니다. 일반적으로 IoU가 특정 임계값(예: 0.5) 이상이면 정확한 탐지로 간주하고, Precision과 Recall을 계산한 후 AP를 산출합니다. 이후 모든 클래스에 대해 AP를 구하고, 이를 평균 낸 값이 최종적인 mAP 값이 됩니다.
- mAP는 객체 인식 모델의 전반적인 성능을 평가하는 핵심 지표로 활용되며, 모델이 다양한 객체를 얼마나 정확하게 탐지하는지 비교할 때 사용됩니다. 특히, YOLO, SSD, Faster R-CNN 등 다양한 객체 탐지 모델에서 성능을 평가하는 기준으로 사용되며, mAP가 높을수록 모델이 정확하게 객체를 인식한다고 볼 수 있습니다.
