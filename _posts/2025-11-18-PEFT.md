---
layout: post
title:  "LoRA 훈련의 90%를 결정하는 '캡셔닝'의 모든 것"
date:   2025-11-18 14:00:00 +0900
categories: [Deep Learning, PEFT]
tags: [PEFT, BERT, GPT, NLP, LLM, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)
image:
    path: https://d3lkc3n5th01x7.cloudfront.net/wp-content/uploads/2023/05/15213322/Parameter-efficient-Fine-tuning.png
description: "🟣 Low-Rank Adapter(LoRA) 기법을 통해서 Diffusion 모델을 파인튜닝해보기"

---

### 1. 딜레마: AI에게 "맛있는 질감"을 어떻게 가르칠까?

"AI 푸드 포토 스튜디오" 프로젝트의 핵심은 'K-Food Enhancement'다. 즉, AI가 폰카로 찍은 '밋밋한' 음식 사진을 '먹음직스러운' 전문가급 사진으로 바꿔줘야 한다.

우리는 이 "먹음직스러운 스타일"을 LoRA (Low-Rank Adaptation) 기법을 통해 Stable Diffusion 모델에 주입하기로 했다.

여기서 첫 번째 관문이 나온다.

"AI에게 '먹음직스러움'을 어떻게 데이터로 정의해서 가르칠 것인가?"

이것은 "Model-centric"한 문제가 아니라, 훈련 데이터셋을 설계하는 **"Data-centric"**한 문제다. 이 데이터 설계의 90%가 바로 **"캡셔닝(Captioning)"**이다.

### 2. 왜 캡션이 중요한가? (Feat. CLIP)

Stable Diffusion 모델은 크게 두 개의 뇌로 움직인다.

- 언어 뇌 (Text Encoder): CLIP 모델. "a photo of a cat" 같은 텍스트를 AI가 이해하는 숫자 벡터(Vector)로 번역한다.
- 시각 뇌 (UNet): CLIP이 번역한 의미 벡터를 받아서 실제 이미지를 그린다.

LoRA 훈련 시, 우리는 1번 **'언어 뇌(CLIP)'는 완벽히 동결(Freeze)**시킨다. 즉, AI에게 새로운 언어(한글)를 가르치는 것이 불가능하다. AI는 오직 훈련 때 배운 "영어" 어휘만 이해할 수 있다.

우리는 오직 2번 '시각 뇌(UNet)'에만 ohwx_kfood_style이라는 **'새로운 스타일'**을 주입한다.

캡션의 역할은, AI가 ohwx_kfood_style을 배울 때, **"이건 스타일이고, 저건 피사체야"**라고 명확하게 선을 그어주는 **'주석(Comment)'**이다.

### 3. 최악의 시나리오: "스타일 오염 (Style Contamination)"

만약 캡션을 잘못 달면 어떻게 될까?

우리가 "윤기나는 양념치킨" 사진 20장을 모았는데, 공교롭게도 20장 모두 **'검은색 배경(누끼컷)'**에 '깨가 뿌려져' 있었다고 가정하자.

- 나쁜 예 ❌
ohwx_kfood_style, a photo of yangnyeom chicken

AI는 이렇게 학습한다:

> "아! ohwx_kfood_style이라는 건, **'검은 배경'**에 '깨를 뿌린' '양념치킨'을 말하는 거구나!"

결과: AI는 '윤기(Glossy)'라는 순수한 스타일이 아니라, '검은 배경'과 '깨'라는 **'객관적 사실(Confounding Variable)'**까지 몽땅 ohwx_kfood_style의 일부로 오해하고 학습한다.
나중에 이 LoRA로 '나무 테이블 위 떡볶이'를 그리라고 하면, AI가 테이블을 검은색으로 덮어씌우거나 떡볶이 위에 깨를 뿌리는 참사가 발생한다.

### 4. 해결책: "캡션 황금 순서 (The Golden Order)"

이 "스타일 오염"을 막고, 우리가 원하는 '스타일'만 정확히 분리(Disentanglement)시키기 위해, 캡션은 일정한 **'우선순위(Priority)'**를 가지고 작성되어야 한다.

[공식] [1. 트리거 워드], [2. 피사체], [3. 구도/샷], [4. 객관적 묘사], [5. 주관적/스타일 묘사]

AI가 캡션을 앞에서부터 읽으면서 "이건 스타일이 아니야!"라고 하나씩 소거(Eliminate)해 나가도록 유도하는 것이다.

[실전 예제 1] 양념치킨 + 깨 + 검은 배경 (glossy_chicken_008.jpg)

**[4. 객관적 묘사]**에 '깨'와 '검은 배경'을 명시하여, AI가 이것을 스타일(5번)로 오해하지 않도록 방지턱을 세운다.

1. 트리거: ohwx_kfood_style,

2. 피사체: a photo of yangnyeom chicken,

3. 구도: high-angle shot, close-up,

4. 객관적 묘사: on a white platter, sprinkled with sesame seeds, parsley, black background, plain background,

5. 주관적/스타일 묘사: studio lighting, glossy, viscous, masterpiece, best quality

[실전 예제 2] 양념치킨 + 나무 쟁반 (glossy_chicken_006.jpg)

**[4. 객관적 묘사]**에 '나무 쟁반', '검은 그릇'을 명시한다.
**[5. 주관적/스타일 묘사]**에 우리가 주입하려는 '윤기'와 '퀄리티 태그'를 집중시킨다.

1. 트리거: ohwx_kfood_style,

2. 피사체: a photo of yangnyeom chicken,

3. 구도: high-angle shot, close-up,

4. 객관적 묘사: in a black bowl, on a wooden tray, with a fork,

5. 주관적/스타일 묘사: glossy, viscous, shiny sauce, oily, studio lighting, masterpiece, best quality, ultra realistic, food photography

5. 결론: 캡셔닝은 'AI 조련'이다

"특징이 없으면 캡션을 생략"하는 것이 최악의 선택이다.
'평범한 조명'이라면 neutral lighting, flat lighting이라고 명시해주어야, AI가 ohwx_kfood_style을 '평범한 조명'이라고 오해하지 않는다. (이것이 "퀄리티 태그"를 masterpiece로 굽는 것만큼 중요하다)

캡셔닝은 AI를 위한 '주석'이자 '가드레일'이다.
이 지루한 '데이터 정제(Data Hygiene)' 작업이 모델의 성능 90%를 결정한다. 이것이 바로 'Data-centric AI'의 핵심이다.