---
layout: post
title:  "10-(3) Attention 메커니즘이 Seq2Seq 모델의 어떤 문제를 해결하는 데 도움이 되나요?"
date:   2025-08-08 13:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [NLP, Attetion, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)

---


## 🟢 TBU
---
#### ⚪ TBU..

## 🟢 예시 답안 (코드잇 제공)
> 
- Attention 메커니즘은 Seq2Seq(Sequence-to-Sequence) 모델이 가지는 중요한 한계를 해결하는 데 큰 도움을 줍니다. 
- 기본적인 Seq2Seq 모델은 입력 시퀀스를 인코더가 하나의 고정된 벡터로 요약하고, 디코더는 이 벡터만을 바탕으로 전체 출력을 생성합니다. 그런데 입력 문장이 길어질수록, 인코더가 모든 정보를 하나의 벡터에 압축하는 데 한계가 생기고, 그로 인해 디코더는 필요한 문맥 정보를 충분히 받지 못하게 됩니다. 이로 인해 성능이 떨어지거나 문장의 앞부분은 잘 예측하면서도 뒷부분은 틀리는 문제가 자주 발생합니다. 
- 이때 Attention 메커니즘을 적용하면, 디코더는 인코더의 전체 hidden state 중에서 매 시점에 필요한 부분에 '집중(attend)'해서 정보를 가져올 수 있게 됩니다. 즉, 입력 시퀀스 전체를 한꺼번에 압축하는 대신, 각 출력 단어를 생성할 때마다 입력 시퀀스의 특정 위치에 더 많은 가중치를 두고 참고할 수 있습니다. 
- 이 덕분에 모델은 긴 문장에서도 필요한 문맥 정보를 유연하게 반영할 수 있고, 장기 의존성 문제(long-term dependency)를 완화할 수 있습니다. 결과적으로 번역, 문장 생성, 질의응답 같은 자연어 처리 과제에서 Seq2Seq 모델의 성능이 크게 향상됩니다.