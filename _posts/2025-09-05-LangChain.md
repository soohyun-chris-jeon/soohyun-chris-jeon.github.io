---
layout: post
title:  "13-(1) LangChain을 사용해 RAG 시스템을 구축할 때 어떤 주요 구성 요소들이 필요하고, 각각 어떤 역할을 하나요?"
date:   2025-09-05 10:00:00 +0900
categories: [LLM, LangChain]
tags: [RAG, LangChain, PEFT, BERT, GPT, NLP, LLM, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)
image:
    path: assets/img/RAG.png

---

## 🟢 LangChain을 사용해 RAG 시스템을 구축할 때 어떤 주요 구성 요소들이 필요하고, 각각 어떤 역할을 하나요?

드디어 RAG이라는 개념이 등장했다. 

개인적으로 이 개념이 등장하게 된 배경이 중요하다고 생각하는데 이번 기회에 RAG 개념을 전반적으로 짚고 넘어가고자 한다.

거대 데이터로 사전 학습된 LLM에 대해서 사용자가 활용도를 높이기 위해서 **배우지 않은 정보를 학습시키는 일**이 매우 중요하다.

전통적인 딥러닝에서는 fine-tuning 기법을 활용하여 LLM에게 학습시켰지만 잦은 업데이트 주기와 fine-tuning 비용 등을 고려했을때 새로운 방법이 필요햇다.

그래서 등장하게 된 것이 RAG⭐이다.

RAG의 한글 번역을 생각해보면 '검색 증강 생성'인데, 말 그대로 검색을 통해서 증강하는 방식인 것이다.

LangChain으로 RAG 시스템을 만드는 것은 마치 'AI에게 외부 자료를 참고해서 답변하게 만드는 과정'을 자동화하는 것과 같다. 

이 과정은 크게 **데이터 인덱싱(Indexing)** 단계와 **검색 및 생성(Retrieval and Generation)** 단계로 나뉩니다.


---
## 🟢 RAG 시스템의 주요 구성 요소 (LangChain 기준)

### ⚪ 0. LangChain이란? 

> LangChain은 LLM(거대 언어 모델)을 사용해서 애플리케이션을 더 쉽고 강력하게 만들 수 있도록 도와주는 **'개발 프레임워크'**이다.

LLM 자체는 똑똑한 '엔진'이지만, 이 엔진만으로는 자동차를 만들 수 없고 핸들, 바퀴, 의자 같은 여러 부품을 조립해야 비로소 자동차가 된다. LangChain은 LLM이라는 엔진에 연결할 수 있는 **다양한 부품(컴포넌트)들을 미리 만들어 놓은 개발 도구**이라고 생각하면 된다.. 

![lang](https://dongmin-sim.github.io/assets/posts/ai/langchain/langchain-do.png)


### ⚪ 1. 데이터 인덱싱 (Indexing): AI가 참고할 인덱스 만들기

이 단계는 AI가 질문에 답하기 전에 미리 참고할 자료들을 정리하고 인덱스 만드는 과정입니다.

#### **(1). Document Loaders (문서 로더)**
* **역할**: RAG 시스템에 필요한 원본 데이터(PDF, 웹사이트, Notion, CSV 등)를 불러오는 첫 관문입니다. LangChain은 텍스트 파일, 웹페이지, PDF, YouTube 스크립트 등 다양한 형태의 데이터를 로드할 수 있는 수많은 종류의 `Document Loader`를 제공합니다.

#### **(2). Text Splitters (텍스트 분할기)**
* **역할**: 로드한 문서는 LLM이 처리하기에는 너무 깁니다. `Text Splitter`는 긴 문서를 의미 있는 작은 덩어리(chunk)로 잘라주는 역할을 합니다. 너무 작게 자르면 의미가 손실되고, 너무 크게 자르면 검색 효율이 떨어지기 때문에, 적절한 크기로 자르는 것이 중요합니다. (e.g., `RecursiveCharacterTextSplitter`)

#### **(3). Embeddings (임베딩 모델)**
* **역할**: 잘라낸 텍스트 덩어리(chunk)들을 컴퓨터가 이해할 수 있는 숫자 벡터(vector)로 변환하는 역할을 합니다. 의미가 비슷한 텍스트는 벡터 공간에서 가까운 위치에 존재하게 됩니다. OpenAI, Hugging Face 등 다양한 임베딩 모델을 LangChain을 통해 쉽게 사용할 수 있습니다.

#### **(4). Vector Stores (벡터 저장소)**
* **역할**: 임베딩된 벡터들을 효율적으로 저장하고, 나중에 질문이 들어왔을 때 유사한 벡터를 빠르게 검색할 수 있도록 인덱싱하는 '벡터 도서관'입니다. Chroma, FAISS, Pinecone 등 다양한 종류의 `Vector Store`를 지원하며, 로컬 환경이나 클라우드에 구축할 수 있습니다.

---

### ⚪ 2. 검색 및 생성 (Retrieval and Generation)

이 단계는 실제 사용자의 질문이 들어왔을 때, 인덱싱된 데이터를 활용해 답변을 생성하는 과정입니다.

#### **5. Retriever (리트리버)**
* **역할**: 사용자의 질문을 받아서, 이를 먼저 벡터로 변환한 뒤 Vector Store에서 의미적으로 가장 유사한(관련성이 높은) 텍스트 덩어리(chunk)들을 찾아오는 역할을 합니다. 단순히 유사도만 검색하는 것 외에도 다양한 고급 검색 전략을 구사할 수 있습니다.

#### **6. LLM (언어 모델)**
* **역할**: RAG의 핵심 두뇌입니다. 리트리버가 찾아온 **'참고 자료(context)'**와 사용자의 **'원본 질문(question)'**을 함께 입력받아, 이 정보를 바탕으로 최종 답변을 생성합니다. LangChain을 통해 OpenAI의 GPT, Anthropic의 Claude 등 다양한 LLM을 쉽게 연동할 수 있습니다.

#### **7. Chains (체인)**
* **역할**: 위 모든 구성 요소들을 순서대로 연결하여 하나의 완성된 파이프라인으로 묶어주는 접착제 역할을 합니다. 예를 들어, `RetrievalQA` 체인은 질문을 받고 -> 리트리버로 문서를 검색하고 -> LLM에 전달하여 답변을 생성하는 전체 과정을 하나로 통합하여 편리하게 실행할 수 있게 해줍니다.

### ⚪ 정리
LangChain 기반 RAG 시스템은 다음과 같은 pipeline 구조로 이해할 수 있다:

> Document Loader → Text Splitter → Embedding → Vector Store(Retriever) → LLM → 답변

이처럼 LangChain은 RAG 시스템 구축에 필요한 각 단계를 모듈화된 '구성 요소'로 제공하여, 개발자가 각 부분을 레고처럼 조립하고 필요에 따라 교체하면서 자신만의 RAG 애플리케이션을 빠르고 유연하게 만들 수 있도록 도와주는 프레임워크이다.



## 🟢 예시 답안 (코드잇 제공)


>LangChain을 사용해 RAG 시스템을 구축할 때는 몇 가지 핵심 구성 요소들이 필요합니다. 각각의 구성 요소는 시스템 내에서 중요한 역할을 하며, 서로 유기적으로 연결되어 작동합니다.<br><br>첫 번째로 필요한 건 문서 로더(Document Loader)입니다. 이는 외부에서 정보를 가져오는 역할을 합니다. 예를 들어 PDF, 웹페이지, 텍스트 파일, 데이터베이스 등 다양한 소스에서 문서를 읽어들여 시스템이 활용할 수 있는 형태로 변환합니다.<br><br>다음은 텍스트 분할기(Text Splitter)입니다. 로딩된 문서가 너무 길면 검색 효율이 떨어지기 때문에, 문서를 일정 길이로 나누는 작업이 필요합니다. 이 과정을 통해 각 문서 조각이 임베딩과 검색에 적합한 형태가 됩니다.<br><br>그다음은 임베딩 모델(Embedding Model)입니다. 문서 조각이나 사용자 질문을 숫자 벡터로 변환하는 역할을 하며, 이를 통해 문서 간 의미적 유사도를 계산할 수 있게 됩니다. OpenAI, Cohere, Hugging Face 모델 등 다양한 임베딩 옵션을 사용할 수 있습니다.<br><br>변환된 벡터는 벡터 데이터베이스(Vector Store)에 저장됩니다. 이 데이터베이스는 나중에 사용자 질문이 들어왔을 때, 가장 관련 있는 문서들을 빠르게 검색하는 역할을 합니다. 대표적으로 FAISS, Pinecone, Weaviate 같은 솔루션이 많이 사용됩니다.<br><br>이제 검색된 문서 조각들을 바탕으로, 프롬프트 템플릿(Prompt Template)이 작동합니다. 사용자의 질문과 관련 문서를 조합해 생성 모델에 전달할 수 있는 입력 문장을 구성하는 단계입니다. 이 프롬프트가 얼마나 잘 구성되느냐에 따라 생성 결과의 품질이 달라질 수 있습니다.<br><br>마지막으로 LLM(Language Model), 즉 생성기 역할을 하는 모델이 사용자의 질문과 관련 문서를 기반으로 응답을 생성합니다. 이 과정을 Chain 또는 Agent 형태로 구성하여, 전체 RAG 파이프라인이 일관되게 작동하도록 만들 수 있습니다.<br><br>정리하면, LangChain을 활용한 RAG 시스템에서는 문서 로딩, 텍스트 분할, 임베딩, 벡터 저장소, 프롬프트 구성, LLM 응답 생성까지 일련의 흐름이 필요하고, 각각의 구성 요소가 자연스럽게 연결되어야 완성도 높은 응답을 생성할 수 있습니다.