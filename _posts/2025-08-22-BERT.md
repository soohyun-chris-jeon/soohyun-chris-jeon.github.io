---
layout: post
title:  "11-(3) BERT와 GPT 이후 등장한 주요 사전학습 모델에는 어떤 것들이 있으며, 특징은 무엇인가요? 구글링 등을 통해 자유롭게 리서치해서 정리해보세요."
date:   2025-08-22 15:00:00 +0900
categories: [Codeit AI 3기, Weekly Paper]
tags: [BERT, GPT, NLP, Trasnformer, Deep Learning, AI]
comments: true     # 댓글 기능 사용 (옵션)

---


## 🟢 TBU
---
#### ⚪ TBU..

## 🟢 예시 답안 (코드잇 제공)
> 
1. RoBERTa (Robustly Optimized BERT Approach)
RoBERTa는 BERT를 기반으로 하되, 학습 방법을 개선해 성능을 끌어올린 모델입니다.
- BERT보다 더 많은 데이터와 긴 학습 시간으로 훈련됨.
- NSP(Next Sentence Prediction) 태스크를 제거하고, 순수한 Masked Language Modeling만 사용.
- 큰 배치 사이즈와 학습률, 다양한 시퀀스 길이 사용.
- BERT와 구조는 동일하지만, 더 정교한 학습 설정으로 대부분의 NLP 태스크에서 성능 향상.<br><br>
2. GPT-2 / GPT-3  
GPT 시리즈는 OpenAI에서 발표한 초거대 언어 생성 모델입니다.  
- GPT-2는 1.5B 파라미터, GPT-3는 175B 파라미터 규모.
- 사전학습만으로도 few-shot / zero-shot 학습이 가능하다는 점을 보여줌.
- 단일 모델로 다양한 언어 생성 작업을 수행할 수 있어 범용성이 매우 높음.
- GPT-3는 API 형태로 제공되며, 텍스트 생성, 문서 작성, 요약, 대화 시스템 등에서 활용.